{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common natural language processing tasks and techniques"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most natural language processing tasks, the text to be processed, must be broken down, examined, and the results stored or cross referenced with rules and data sets. These tasks, allows the programmer to derive the meaning or intent or only the frequency of terms and words in a text."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks common to NLP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different ways to analyse a text you are working on. There are tasks you can perform and through these tasks you are able to gauge an understanding of the text and draw conclusions. You usually carry out these tasks in a sequence."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably the first thing most NLP algorithms have to do is to split the text into tokens, or words. While this sounds simple, having to account for punctuation and different languages' word and sentence delimiters can make it tricky. You might have to use various methods to determine demarcations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings are a way to convert your text data numerically. Embeddings are done in a way so that words with a similar meaning or words used together cluster together."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing & Part-of-speech Tagging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing is recognizing what words are related to each other in a sentence - for instance the quick red fox jumped is an adjective-noun-verb sequence that is separate from the lazy brown dog sequence."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word and Phrase Frequencies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A useful procedure when analyzing a large body of text is to build a dictionary of every word or phrase of interest and how often it appears. The phrase the quick red fox jumped over the lazy brown dog has a word frequency of 2 for the."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an example text where we count the frequency of words. Rudyard Kipling's poem The Winners contains the following verse:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What the moral? Who rides may read.\n",
    "\n",
    "When the night is thick and the tracks are blind\n",
    "\n",
    "A friend at a pinch is a friend, indeed,\n",
    "\n",
    "But a fool to wait for the laggard behind.\n",
    "\n",
    "Down to Gehenna or up to the Throne,\n",
    "\n",
    "He travels the fastest who travels alone."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As phrase frequencies can be case insensitive or case sensitive as required, the phrase a friend has a frequency of 2 and the has a frequency of 6, and travels is 2."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-grams"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A text can be split into sequences of words of a set length, a single word (unigram), two words (bigrams), three words (trigrams) or any number of words (n-grams)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance the quick red fox jumped over the lazy brown dog with a n-gram score of 2 produces the following n-grams:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    1 the quick\n",
    "\n",
    "    2 quick red\n",
    "\n",
    "    3 red fox\n",
    "\n",
    "    4 fox jumped\n",
    "\n",
    "    5 jumped over\n",
    "\n",
    "    6 over the\n",
    "\n",
    "    7 the lazy\n",
    "\n",
    "    8 lazy brown\n",
    "    \n",
    "    9 brown dog\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be easier to visualize it as a sliding box over the sentence. Here it is for n-grams of 3 words, the n-gram is in bold in each sentence:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    1 the quick red fox jumped over the lazy brown dog\n",
    "\n",
    "    2 the quick red fox jumped over the lazy brown dog\n",
    "\n",
    "    3 the quick red fox jumped over the lazy brown dog\n",
    "\n",
    "    4 the quick red fox jumped over the lazy brown dog\n",
    "\n",
    "   5  the quick red fox jumped over the lazy brown dog\n",
    "\n",
    "   6 the quick red fox jumped over the lazy brown dog\n",
    "\n",
    "   7 the quick red fox jumped over the lazy brown dog\n",
    "\n",
    "   8 the quick red fox jumped over the lazy brown dog\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noun phrase Extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most sentences, there is a noun that is the subject, or object of the sentence. In English, it is often identifiable as having 'a' or 'an' or 'the' preceding it. Identifying the subject or object of a sentence by 'extracting the noun phrase' is a common task in NLP when attempting to understand the meaning of a sentence."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n the sentence \"I cannot fix on the hour, or the spot, or the look or the words, which laid the foundation. It is too long ago. I was in the middle before I knew that I had begun.\", can you identify the noun phrases?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sentence the quick red fox jumped over the lazy brown dog there are 2 noun phrases: quick red fox and lazy brown dog"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sentence or text can be analysed for sentiment, or how positive or negative it is. Sentiment is measured in polarity and objectivity/subjectivity. Polarity is measured from -1.0 to 1.0 (negative to positive) and 0.0 to 1.0 (most objective to most subjective)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later you'll learn that there are different ways to determine sentiment using machine learning, but one way is to have a list of words and phrases that are categorized as positive or negative by a human expert and apply that model to text to calculate a polarity score. Can you see how this would work in some circumstances and less well in others?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inflection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inflection enables you to take a word and get the singular or plural of the word."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lemma is the root or headword for a set of words, for instance flew, flies, flying have a lemma of the verb fly."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordNet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordNet is a database of words, synonyms, antonyms and many other details for every word in many different languages. It is incredibly useful when attempting to build translations, spell checkers, or language tools of any type."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP Libraries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, you don't have to build all of these techniques yourself, as there are excellent Python libraries available that make it much more accessible to developers who aren't specialized in natural language processing or machine learning. The next lessons include more examples of these, but here you will learn some useful examples to help you with the next task."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise - using TextBlob library"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a library called TextBlob as it contains helpful APIs for tackling these types of tasks. TextBlob \"stands on the giant shoulders of NLTK and pattern, and plays nicely with both.\" It has a considerable amount of ML embedded in its API."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When attempting to identify noun phrases, TextBlob offers several options of extractors to find noun phrases."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at ConllExtractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\user/nltk_data'\n    - 'c:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\Arewads\\\\nltk_data'\n    - 'c:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\Arewads\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\Arewads\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m user_input \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m> \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m user_input_blob \u001b[39m=\u001b[39m TextBlob(user_input, np_extractor\u001b[39m=\u001b[39mextractor)  \u001b[39m# note non-default extractor specified\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m np \u001b[39m=\u001b[39m user_input_blob\u001b[39m.\u001b[39;49mnoun_phrases\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\Arewads\\lib\\site-packages\\textblob\\decorators.py:24\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mif\u001b[39;00m obj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     23\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n\u001b[1;32m---> 24\u001b[0m value \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(obj)\n\u001b[0;32m     25\u001b[0m \u001b[39mreturn\u001b[39;00m value\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\Arewads\\lib\\site-packages\\textblob\\blob.py:483\u001b[0m, in \u001b[0;36mBaseBlob.noun_phrases\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    479\u001b[0m \u001b[39m@cached_property\u001b[39m\n\u001b[0;32m    480\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnoun_phrases\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    481\u001b[0m     \u001b[39m\"\"\"Returns a list of noun phrases for this blob.\"\"\"\u001b[39;00m\n\u001b[0;32m    482\u001b[0m     \u001b[39mreturn\u001b[39;00m WordList([phrase\u001b[39m.\u001b[39mstrip()\u001b[39m.\u001b[39mlower()\n\u001b[1;32m--> 483\u001b[0m                     \u001b[39mfor\u001b[39;00m phrase \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnp_extractor\u001b[39m.\u001b[39;49mextract(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw)\n\u001b[0;32m    484\u001b[0m                     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(phrase) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\Arewads\\lib\\site-packages\\textblob\\en\\np_extractors.py:66\u001b[0m, in \u001b[0;36mConllExtractor.extract\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract\u001b[39m(\u001b[39mself\u001b[39m, text):\n\u001b[0;32m     65\u001b[0m     \u001b[39m'''Return a list of noun phrases (strings) for body of text.'''\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m     sentences \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mtokenize\u001b[39m.\u001b[39;49msent_tokenize(text)\n\u001b[0;32m     67\u001b[0m     noun_phrases \u001b[39m=\u001b[39m []\n\u001b[0;32m     68\u001b[0m     \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m sentences:\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\Arewads\\lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msent_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[39m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[39m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     tokenizer \u001b[39m=\u001b[39m load(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtokenizers/punkt/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlanguage\u001b[39m}\u001b[39;49;00m\u001b[39m.pickle\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    107\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\Arewads\\lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<<Loading \u001b[39m\u001b[39m{\u001b[39;00mresource_url\u001b[39m}\u001b[39;00m\u001b[39m>>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[39m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[39m=\u001b[39m _open(resource_url)\n\u001b[0;32m    752\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[39m=\u001b[39m opened_resource\u001b[39m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\Arewads\\lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[39m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[39mif\u001b[39;00m protocol \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnltk\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, path \u001b[39m+\u001b[39;49m [\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[39melif\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[39m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, [\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mopen()\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\Arewads\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\user/nltk_data'\n    - 'c:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\Arewads\\\\nltk_data'\n    - 'c:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\Arewads\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\Arewads\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob.np_extractors import ConllExtractor\n",
    "# import and create a Conll extractor to use later \n",
    "extractor = ConllExtractor()\n",
    "\n",
    "# later when you need a noun phrase extractor:\n",
    "user_input = input(\"> \")\n",
    "user_input_blob = TextBlob(user_input, np_extractor=extractor)  # note non-default extractor specified\n",
    "np = user_input_blob.noun_phrases                                    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenge - improving your bot with NLP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous lesson you built a very simple Q&A bot. Now, you'll make Marvin a bit more sympathetic by analyzing your input for sentiment and printing out a response to match the sentiment. You'll also need to identify a noun_phrase and ask about it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your steps when building a better conversational bot:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    1 Print instructions advising the user how to interact with the bot\n",
    "    2 Start loop\n",
    "       i Accept user input\n",
    "       ii If user has asked to exit, then exit\n",
    "       iii Process user input and determine appropriate sentiment response\n",
    "       iv If a noun phrase is detected in the sentiment, pluralize it and ask for more input on that topic\n",
    "      v  Print response\n",
    "    3 loop back to step 2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the code snippet to determine sentiment using TextBlob. Note there are only four gradients of sentiment response (you could have more if you like):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob.np_extractors import ConllExtractor\n",
    "\n",
    "if user_input_blob.polarity <= -0.5:\n",
    "  response = \"Oh dear, that sounds bad. \"\n",
    "elif user_input_blob.polarity <= 0:\n",
    "  response = \"Hmm, that's not great. \"\n",
    "elif user_input_blob.polarity <= 0.5:\n",
    "  response = \"Well, that sounds positive. \"\n",
    "elif user_input_blob.polarity <= 1:\n",
    "  response = \"Wow, that sounds great. \""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is some sample output to guide you (user input is on the lines with starting with >):"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello, I am Marvin, the friendly robot.\n",
    "\n",
    "You can end this conversation at any time by typing 'bye'\n",
    "\n",
    "After typing each answer, press 'enter'\n",
    "\n",
    "How are you today?\n",
    "\n",
    "> I am ok\n",
    "\n",
    "Well, that sounds positive. Can you tell me more?\n",
    "\n",
    "> I went for a walk and saw a lovely cat\n",
    "\n",
    "Well, that sounds positive. Can you tell me more about lovely cats?\n",
    "\n",
    "> cats are the best. But I also have a cool dog\n",
    "\n",
    "Wow, that sounds great. Can you tell me more about cool dogs?\n",
    "\n",
    "> I have an old hounddog but he is sick\n",
    "\n",
    "Hmm, that's not great. Can you tell me more about old hounddogs?\n",
    "\n",
    "> bye\n",
    "\n",
    "It was nice talking to you, goodbye!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\user/nltk_data'\n    - 'c:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\Arewads\\\\nltk_data'\n    - 'c:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\Arewads\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\Arewads\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m user_input \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mWhat do you want to talk about? \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[39m# Generate a response\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m response \u001b[39m=\u001b[39m generate_response(user_input)\n\u001b[0;32m     37\u001b[0m \u001b[39m# Print the response\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[39mprint\u001b[39m(response)\n",
      "Cell \u001b[1;32mIn[8], line 10\u001b[0m, in \u001b[0;36mgenerate_response\u001b[1;34m(user_input)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_response\u001b[39m(user_input):\n\u001b[0;32m      9\u001b[0m   \u001b[39m# Tokenize the user's input\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m   tokens \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mword_tokenize(user_input)\n\u001b[0;32m     12\u001b[0m   \u001b[39m# Identify any trigger words\u001b[39;00m\n\u001b[0;32m     13\u001b[0m   trigger_words_found \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\Arewads\\lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    130\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\Arewads\\lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msent_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[39m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[39m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     tokenizer \u001b[39m=\u001b[39m load(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtokenizers/punkt/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlanguage\u001b[39m}\u001b[39;49;00m\u001b[39m.pickle\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    107\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\Arewads\\lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<<Loading \u001b[39m\u001b[39m{\u001b[39;00mresource_url\u001b[39m}\u001b[39;00m\u001b[39m>>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[39m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[39m=\u001b[39m _open(resource_url)\n\u001b[0;32m    752\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[39m=\u001b[39m opened_resource\u001b[39m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\Arewads\\lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[39m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[39mif\u001b[39;00m protocol \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnltk\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, path \u001b[39m+\u001b[39;49m [\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[39melif\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[39m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, [\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mopen()\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\Arewads\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\user/nltk_data'\n    - 'c:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\Arewads\\\\nltk_data'\n    - 'c:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\Arewads\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\user\\\\miniconda3\\\\envs\\\\Arewads\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import textblob\n",
    "\n",
    "# Create a list of trigger words\n",
    "trigger_words = [\"why\", \"how\"]\n",
    "\n",
    "# Create a function to generate a response\n",
    "def generate_response(user_input):\n",
    "  # Tokenize the user's input\n",
    "  tokens = nltk.word_tokenize(user_input)\n",
    "\n",
    "  # Identify any trigger words\n",
    "  trigger_words_found = []\n",
    "  for word in tokens:\n",
    "    if word in trigger_words:\n",
    "      trigger_words_found.append(word)\n",
    "\n",
    "  # If no trigger words are found, generate a random response\n",
    "  if len(trigger_words_found) == 0:\n",
    "    return textblob.TextBlob(\"I don't know what you mean.\").generate()\n",
    "\n",
    "  # Otherwise, generate a response based on the trigger word\n",
    "  for trigger_word in trigger_words_found:\n",
    "    if trigger_word == \"why\":\n",
    "      return textblob.TextBlob(\"I don't know why.\").generate()\n",
    "    elif trigger_word == \"how\":\n",
    "      return textblob.TextBlob(\"I don't know how.\").generate()\n",
    "\n",
    "# Start the chat bot\n",
    "while True:\n",
    "  # Get the user's input\n",
    "  user_input = input(\"What do you want to talk about? \")\n",
    "\n",
    "  # Generate a response\n",
    "  response = generate_response(user_input)\n",
    "\n",
    "  # Print the response\n",
    "  print(response)\n",
    "\n",
    "  # Check if the user wants to end the chat\n",
    "  if user_input == \"bye\":\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from textblob import TextBlob\n",
    "from textblob.np_extractors import ConllExtractor\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "extractor = ConllExtractor()\n",
    "\n",
    "def main():   \n",
    "    print(\"Hello, I am Marvin, the friendly robot.\")\n",
    "    print(\"You can end this conversation at any time by typing 'bye'\")    \n",
    "    print(\"After typing each answer, press 'enter'\")\n",
    "    print(\"How are you today?\")\n",
    "\n",
    "    while True:\n",
    "        # wait for the user to enter some text\n",
    "        user_input = input(\"> \")\n",
    "\n",
    "        if user_input.lower() == \"bye\":            \n",
    "            # if they typed in 'bye' (or even BYE, ByE, byE etc.), break out of the loop\n",
    "            break\n",
    "        else:\n",
    "            # Create a TextBlob based on the user input. Then extract the noun phrases\n",
    "            user_input_blob = TextBlob(user_input, np_extractor=extractor)                        \n",
    "            np = user_input_blob.noun_phrases                                    \n",
    "            response = \"\"\n",
    "            if user_input_blob.polarity <= -0.5:\n",
    "                response = \"Oh dear, that sounds bad. \"\n",
    "            elif user_input_blob.polarity <= 0:\n",
    "                response = \"Hmm, that's not great. \"\n",
    "            elif user_input_blob.polarity <= 0.5:\n",
    "                response = \"Well, that sounds positive. \"\n",
    "            elif user_input_blob.polarity <= 1:\n",
    "                response = \"Wow, that sounds great. \"\n",
    "\n",
    "            if len(np) != 0:\n",
    "                # There was at least one noun phrase detected, so ask about that and pluralise it\n",
    "                # e.g. cat -> cats or mouse -> mice\n",
    "                response = response + \"Can you tell me more about \" + np[0].pluralize() + \"?\"\n",
    "            else:\n",
    "                response = response + \"Can you tell me more?\"\n",
    "            print(response)\n",
    "    \n",
    "    print(\"It was nice talking to you, goodbye!\")\n",
    "\n",
    "# Start the program\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Arewads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
